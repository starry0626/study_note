## 如何提高视频理解中的对视频的准确感知, 减少幻觉?
对于视频理解而言, 由于其信息密度较低, 包含大量的无关信息, 感知通常情况下比逐步进行推理更为重要. 

解决该问题大体可以有以下两种思路:
- 从推理流程入手
- 从RL训练入手
### 推理流程
这种方向的研究更多, 主要可分为下面几种方法
- 在推理流程中加入evidence(时序时间段或者空间box)
- 视觉聚焦来提供分辨率更高的信息
- 多轮推理反复调取原视频(agentic, 更适配长视频)
  
这种方法的缺点在于想要是模型学会复杂的推理范式, **需要数据量更大, 训练也更不稳定**; 工程实现难度大; **推理消耗的token数量显著增加**.

一个应对需要数据量更大, 训练也更不稳定的问题的方法在于固定下训练时的推理流程(videochat-r1.5, videoauto-r1), 但这会限制推理流程, 并不适用于所有方法.

一个可能有效的思路为直接将视觉信息插入到思维链中替代或者补充对应的对视频的文字描述.
1. 这相较于普通的视觉聚焦是否能有提升
2. 如何训练/推理

### RL训练
这部分的研究较少
现在来看一个主要的思路是通过**构造反例数据**加入到训练中以促使模型关注相应的点. 
- video-r1的反例数据为将帧的顺序打乱, 采用的训练方法为对正例数据与反例数据分别采样, 若正例准确率高于反例则获得额外奖励.
- See Less, See Right采用图表, 构造正反例, 正例仅保留关键信息, 反例去除关键信息, 通过KL散度衡量原始输入下与正例反例输入下分别的差异并加入奖励中.



